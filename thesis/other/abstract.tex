\section*{\LARGE Abstract}
With the increasing ubiquity of voice input systems to computers users rely on robust speech recognition algorithms to process these intents. The first step and key component to automatic speech recognition is language detection. Without automatic language detection all subsequent recognition steps will fail because they can neither parse the speech utterances correctly nor apply proper grammar.

A second trend in computer science is the successful application of deep neural networks on a variety of problems. In this thesis, we present a hybrid neural network system using deep learning techniques for automatic language identification for speech audio samples. Specifically, we apply convolutional recurrent neural networks on human speech inputs and evaluate their robustness in different environments.

Convolutional neural networks have shown great promise within the computer vision research community. Therefore, we base our research on established model architectures such as the Inception network\cite{szegedy2015going} and transfer our audio-based research task into the image processing domain. We study the effectiveness of spectrogram images as a valuable input feature. We discuss additional audio representations and related work for speech processing systems.

Deep learning systems benefit greatly from the availability of large-scale datasets. We train our models on more than \num{1000} hours of speech audio in six different languages: English, German, French, Spanish, Mandarin Chinese and Russian. We collect and process this data from speeches and session from the European Parliament as well as from news channels such as the BBC hosted on YouTube.

Our best performing convolutional recurrent neural network scores a top accuracy and F1 score of \SI{96}{\percent} on the news dataset. With this approach, we report a constant improvement over various baseline convolutional neural networks. We evaluate our models in diverse noisy scenarios with data augmented to include white noise, crackling noise, and background music and observe a decrease in accuracy by 5 percentage points (p.p.), 3 p.p., 7 p.p, respectively. On the smaller EU dataset we achieve an accuracy and F1 score of~\SI{98}{\percent}.
