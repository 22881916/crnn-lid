\section{Experiments and Evaluation} 
\label{sec:evaluation}
In this chapter we show and discuss the results of training the outlined neural network architecture for spoken language identification. We introduce several performance metrics and present the results evaluated on our system.
Further, we experiment with modified model architectures to optimize our model for noise robustness. To assess the real world performance of the neural network we augment our data to simulated various noisy environments. We show the classification performance of our approach by discussing the inter language discrimination and extensibility to other languages.     

\subsection{Setup} 
\label{sec:setup}

\subsubsection{Hardware Resources}
\label{sec:hardware}
	In order to facilitate Keras' and TensorFlow's hardware-accelerated computation we ran of training on CUDA\footnote{\url{https://developer.nvidia.com/cuda-zone}, accessed 30.01.2017} compatible GPU machines. All trainings and experiments were executed on two CUDA enabled machines belonging to the Internet Technologies and Systems chair. Details can be found in table \ref{tab:hardware}.
		
	\begin{table}[h]
	\centering
	\begin{tabularx}{\textwidth}{lll}
	\toprule
	  		& Machine A 					& Machine B \\ \midrule
	OS  	& Ubuntu Linux 14.04 		& Ubuntu Linux 16.04 \\
	CPU  	& Intel\textsuperscript{\textregistered} Core\textsuperscript{\texttrademark} i7-4790K @4GHz 	& AMD FX\textsuperscript{\texttrademark}-8370  @ 4GHz \\
	RAM  	& 16GB 						& 32GB \\
	GPU  	& Nvidia GeForce\textsuperscript{\textregistered} GTX 980 	& Nvidia Titan X \\
	VRAM  	& 4GB 						& 12GB \\
	\bottomrule
	\end{tabularx}
	\caption{Hardware resources used in training the neural network.}
	\label{tab:hardware}
	\end{table}

\subsubsection{Data} 
\label{sec:data}
For our performance evaluation we used the European Speech Repository and YouTube News dataset as describe in section \ref{sec:datasets}. Both datasets were preprocessed and converted spectrogram images as described in section \ref{sec:data_processing}. Each spectrogram image represents a non-overlapping ten second duration of source audio file. 
We split both dataset into a training (70\%), validation (20\%) and testing set (10\%) and all files were distributed equally between all four language classes. The amount of samples per language is limited by language with least files to ensure an equal class distribution. The yield of the datasets could be increased by increasing the number of Spanish files. Nonetheless, the European Speech repository yields a total of 19.000 training images files which adds to roughly 53 hours of speech audio. The YouTube News dataset is considerable larger and yields a total of 194.000 training files, or 540 hours. Table \ref{tab:data_splits} contains the detailed dataset splits.

	\begin{table}[]
	\centering
	\begin{tabularx}{\textwidth}{lrr}
	\toprule
	  				& European Speech Repository & YouTube News\\ \midrule
	Training Set    & 18.788						 & 193.432 \\
	Validation Set  & 5.372						 & 55.272 \\
	Test Set        & 2.684						 & 27.632 \\
	\midrule
	Total           & 26.844						 & 276.336 \\
	\bottomrule
	\end{tabularx}
	\caption{As is common in data science we split our dataset into a training, validation and testing set.}
	\label{tab:data_splits}
	\end{table}

Given the European Speech Repository's smaller size we only used it initially confirm the validity of our approach. Since we were satisfied with the results we did not do include it in the extensive robustness tests that we used for the evaluation on the YouTube News dataset. In addition to the original audio we augmented the news dataset with three different background noises to evaluate how well our model would hold out in non ideal, real world situations outside of a new broadcasting studio. For the first experiment we added generic white noise to data. For the second experiment we added noise to simulate an old phone line or bad internet connection during voice chat. Lastly, we added background music to the data. All experiments are described in detail below.


\subsubsection{Training of the Neural Network Model} 
\label{sec:training}
Neural networks have a multitude of hyperparameters that influence the training results drastically. In the following we will briefly explain our choice of hyperparameters and other important training settings.

	\begin{description}
	\item[Optimizer] We employed an Adam\cite{kingma2014adam} optimizer to quickly and efficiently convergence our model. The Adam solver utilizes momentum during gradient updates and to support a quicker convergence. We set the optimizer's parameters $\beta_1$ to 0.9, $\beta_2$ to 0.999, and $\epsilon$ to 1e-08. Overall we found it to be an order of magnitude quicker than using standard stochastic gradient descent (\ac{sgd}). We resorted to SGD finetuning when needed more control over the learning rate schedule and wanted smaller weight updates.
	\item[Weights Initializer] All layer weights are initialized within the range is [0, 1) using Keras' default random Glorot uniform initializer\cite{glorot2010understanding}.
	\item[Data Normalization] The greyscale images are loaded using SciPy and normalized to the [0, 1] range. The shape for all inputs needs to be uniform across all samples and is set to [500, 129, 1], unless otherwise noted. The data loader uses Python generators\footnote{\url{https://docs.python.org/3/glossary.html#term-generator}, accessed 30.01.2017} to keep the system's memory requirements low.
	\item[Learning Rate] We set the initial learning rate to 0.001. Given the Adam optimizer's dynamic learning rate adaption we expect the learning rate to be increase or decreased after every epoch. 
	\item[Batch Size] We specified the batch size depending on the available VRAM of the training machine. We used a value of 64 for Machine A and 128 for Machine B. See section \ref{sec:hardware} for the hardware specifications. 
	\item[Weight Regularization] We employed the L2 norm as a weight regularizer for all convolutional and fully connected layers to improve the models generality. We penalize our loss with a weight decay value of 0.001. 
	\item[Epochs] We limited the model training to a maximum of 50 epochs when using the Adam solver. We usually reach convergence well below this threshold. For training sessions with SGD we increased this considerably. To speed up our workflow we employed an early stopping policy and stopped a training if the validation accuracy and loss did not increase within a ten window.
	\item[Metrics] We observed the loss, accuracy, recall, precession, f1 measure, and equal error rate for both the training and validation set during model training. All values were saved to log files and visualized as graphs in Tensorboard\footnote{\url{https://www.tensorflow.org/how_tos/summaries_and_tensorboard/}, accessed 30.01.2017}.   
	\end{description}


\subsection{Evaluation} 

\subsubsection{Evaluation Metrics} 
\label{sec:metrics}
In this section we discuss the evaluation metrics used throughout our experiments. 

\todo{Multiclass classifier, scikit learn metric package}

\begin{description}
    \item[Accuracy] is a common measure in machine learning and is defined as the ration of correctly classified samples to all samples in the dataset. In the context of language identification this translates as:
     
    	$$
		Accuracy = \frac{\abs{\{\text{correctly identified language samples}\}}}{\abs{\{\text{all language samples}\}}}
		$$
     
    
    \item[Precision and Recall] Precision defines the ratio of retrieved language samples that are correctly identified as belonging to said language. Recall is the fraction of correctly identified language samples to all samples belonging to this language.
    
    	\todo{fix this}
    
    	how many were correctly predicted
    	if all samples belonging to a language were assigned
     
		$$
 		truePositives = \abs{\set{\parbox{0.7\textwidth}{samples belonging to a language which were correctly identified as belonging to said language}}} 
		$$
		\todo{fix set's curly braces}

		$$
 		falsePositives = \abs{\set{\parbox{0.7\textwidth}{samples belonging to a language which were identified as belonging to another language}}} 
		$$
		
		$$
 		falseNegatives = \abs{\set{\parbox{0.7\textwidth}{samples belonging to a language which were incorrectly identified as not belonging to said language}}} 
		$$

	    $$
	    precision = \frac
	      {truePositives}
	      {truePositives + falsePositives}
	    $$
		
		$$
		recall = \frac
			{truePositives}
			{truePositives + falseNegatives}
		$$    


    \item[The F1 Score] is the scaled harmonic mean of precision and recall. It is used to a have combined judgement of recall and precision, since one is generally not interested in assessing one without the other.
    
    	$$
    	F1 = 2 * \frac{precision * recall}{precision + recall}
    	$$
    \item[Equal Error Rate]
    \todo{add or remove}
\end{description}

\subsubsection{Results for EU Speech Repository Dataset}
\label{sec:results_eu}

\subsubsection{Results for YouTube News Dataset}
\label{sec:results_news}

\subsubsection{Inter Language Discrimination} 
\label{sec:lang_discrimination}
\subsubsection{Effect of Audio Duration} 
\label{sec:duration}
\subsubsection{Noise Robustness} 
\label{sec:noise_robustness}
\subsubsection{Background Music Robustness} 
\label{sec:music_robustness}
\subsubsection{Model Extensibility} 
\label{sec:extensibility}
\subsubsection{Visualization} 
\label{sec:visualization}

\subsubsection{Discussion and Comparison} 
\label{sec:comparison}

- evaluierung nur auf 10s snippets nd nicht auf ganzen audio files
- noch bessere perf mit majority voting Ã¼ber mehrere segmente
