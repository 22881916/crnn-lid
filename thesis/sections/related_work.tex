\section{Related Work}
\label{sec:related_work}
In this chapter, we lay out related work concerning neural network designs in general and hybrid networks that are specifically tailored to language identification. Additionally, we highlight related work on input feature representations suitable for machine learning on audio files. Furthermore, we present research on \emph{i-vector systems}, the traditional approach to LID. Finally, we list related work on data augmentation.

\subsection{Convolutional Neural Network Architectures}
With good results in the ImageNet Large Scale Visual Recognition Challenge (ILSVRC)~\cite{ILSVRC15}, AlexNet~\cite{krizhevsky2012imagenet}, VGGNet~\cite{simonyan2014very}, and GoogLeNet/Inception~\cite{szegedy2015going} have become the de facto standards for neural network designs in the computer vision community.

The Visual Geometry Group at Oxford University published their deep neural network design, which they called VGGNet. Simonyan et al. proposed a convolutional neural network architecture with a depth of up to \num{16} or \num{19}~weight layers~\cite{simonyan2014very, Chatfield14}. They presented a thorough evaluation of the recently increasing depth of neural networks by using convolutional layers with $3 \times 3$ kernel sizes and ReLU activations. Their system ended up winning the ILSVRC~2104 challenge.

Szegedy et al. reported on several iterations of Google's convolutional neural network architecture, called GoogLeNet or Inception~\cite{szegedy2015going, szegedy2016rethinking, szegedy2016inception}. These deep and very deep convolutional neural networks repeatedly set the state-of-the-art record for minimal classification errors in the ILSVRC competition. The introduced network architectures have multiple advantages over previous CNN designs such as VGGNet. Google introduced so-called \emph{inception modules} or \emph{mini-networks}, which aim to factorize convolutions with larger filter sizes in order to reduce training times and the number of model parameters. The idea is to replace larger spatial filters (for instance, $5 \times 5$ and $7 \times 7$), which are disproportionally expensive in terms of computation, with less expensive, smaller inception modules without any loss of visual expressiveness. The inception modules are represented as a sequence of $3 \times 3$ convolutions followed by a $1 \times 1$ convolution. In case of replacing $5 \times 5$ filters, the authors were able to achieve a computational speed-up of~\SI{28}{\percent}. The resulting Inception-v2 and Inception-v3 networks consist of \num{42}~layers and are trained using the RMSProp optimizer~\cite{tieleman2012lecture}. Compared to the VGGNet-like networks, the inception networks feature lower overall computational costs, while offering higher accuracy on image vision tasks.

A second innovation introduced by inception networks is the use of a technique called \emph{batch normalization}~\cite{ioffe2015batch}. During training, the parameters of each layer continuously change and, hence, also the value distribution of the respectively next layer's input. This makes training deep neural networks particularly complicated and slows down the training phase by requiring lower learning rates and careful parameter initialization. Szegedy et al. refer to this phenomenon as \emph{internal covariate shift}. Their proposed solution is to normalize the inputs of each mini-batch for the following layer to unit vectors. This results in a number of benefits. Foremost, the authors were able to drastically reduce the time needed for the models to converge. Second, batch normalization enabled them to use higher learning rates without running into the \emph{vanishing} or \emph{exploding gradient problem}. Furthermore, batch normalization acts as model regularizer positively affecting the generalization abilities of the network. In turn, this eliminates the need for dropout layers as regularizers. The authors conclude that by simply adding batch normalization to the convolutional layers of the inception network, they were able to beat the state of the art in the ILSVRC challenge.

Shi et al. proposed an end-to-end trainable neural network for image-based sequence recognition and its application to scene text recognition~\cite{shi2016end}. The authors developed a hybrid neural network consisting of a convolutional part and a recurrent part for \emph{optical character recognition} (OCR). Convolutional layers are used as robust feature extractors for the input images. The resulting feature maps are interpreted as a time sequence of feature vectors, which is fed into a LSTM network to capture the contextual information within the sequence. The authors jointly trained the whole CRNN with a \emph{connectionist temporal classification} loss function (CTC)~\cite{graves2006connectionist} to output a sequence of letters. The best-performing network architecture is a \emph{VGGNet}-based one composed of seven convolutional layers with max pooling followed by a bidirectional LSTM. The paper concludes that the use of batch normalization greatly reduces the training time. Additionally, the authors employed $1 \times 2$-sized rectangular pooling windows instead of conventional square ones. This tweak yielded feature maps of larger width and, hence, longer sequences for the RNN.


\subsection{Spoken-Language Processing Systems}
Much of the research on audio representations and spoken-language processing is rooted in various related research communities: \emph{music information retrieval} (MIR), \emph{automatic speech recognition} (ASR) and \emph{language identification} (LID). Alongside the rising popularity of neural networks within the computer vision community, we can witness their use in audio-based tasks as well. Early LID system integrated and combined shallow neural networks within their i-vector systems (more details in Section~\ref{sec:i-vector-systems})~\cite{gonzalez2014automatic, han2013trap, matejka2014neural, richardson2015unified}. These models usually feature no more than three layers and consist solely of classic neural networks or fully-connected layers. While these systems benefit neither from the more efficient computation of CNNs nor the expressiveness of deeper networks, they were already able to improve on existing systems.

Song et al. reported the use of an end-to-end-trainable, hybrid, convolutional recurrent neural network for automatic speech recognition~\cite{song2015end}. The authors focused on classifying phoneme sequences of the input speech samples. Their proposed network consists of four convolutional layers followed by two fully-connected layers and is finalized by two LSTM layers. The network was jointly trained on the TIMIT dataset using a CTC loss function and grayscale images from mel-filter banks as input. Given the short duration of the individual phonemes, the system operates on audio snippets of~\num{15} to \num{25}~milliseconds. Similar to Shi et al., the authors apply rectangular pooling layers to obtain longer feature vector sequences~\cite{shi2016end}. The reported results compete with traditional Gaussian mixture models and hidden Markov models for ASR tasks.

Amodei et al. presented the Deep Speech~2 system for speech recognition. The system supports English and Mandarin Chinese as input languages~\cite{amodei2015deep}. The CRNN employs only three convolutional layers followed by seven bidirectional RNN or GRU layers. The model was trained end-to-end using a CTC loss function and a sequence of spectrograms of power-normalized audio as input. The system outputs a sequence of graphemes and uses a language model together with beam search to reconstruct words from audio. The authors found that both the LSTM and GRU cells performed similarly well as the RNN layers. GRU cells, however, needed less computations and were less likely to diverge. For the CNN part, the paper evaluated both the use of 1D, time-only domain convolutions and 2D, frequency--time domain convolutions. 2D convolutions fared better, especially with regard to noisy data.
Deep Speech 2 was trained with \num{12000}~hours of English speech and \num{9000}~hours of Mandarin Chinese. Additionally, it used an increased dataset with augmented data both boosting the effective corpus size as well as improving the system's noise robustness. The authors also evaluated the system with accented speech and noisy read-outs in coffee shops, streets, and so on. In both scenarios, the model's performance deteriorated.

\subsection{Methods of Input Data Representation}
There are many different types of audio representations used in spoken-language processing. Many higher-level characteristics of sound relate to the energies of different frequency bands. This explains the utility of time--frequency representations of audio, such as spectrograms, which are frequently used in the literature~\cite{montavon2009deep, dieleman2013multiscale, lee2009unsupervised, wulfing2012unsupervised, henaff2011unsupervised}. Alternatively, classical machine learning systems and i-vector systems usually rely on \emph{mel-frequency cepstral coeffiecient} vectors (MFCC)~\cite{richardson2015unified, dehak2011front, garcia2011analysis} or derivatives thereof such as \emph{perceptual linear prediction coefficients} (PLP)~\cite{gonzalez2014automatic}. Other researchers have evaluated the use of raw waveform audio directly~\cite{dieleman2014end, collobert2016wav2letter}.

Collobert et al. introduced Wav2Letter, an end-to-end convolutional neural network speech recognition system~\cite{collobert2016wav2letter}. The authors evaluated their system with three different input representations: mel-frequency cepstral coeffiecient, spectrograms, and raw waveform data. In their paper, a fully convolutional neural network performed best with MFCC vectors as input. Yet, power spectrograms still outperformed raw waveform inputs. This observation corresponds with that of Dieleman et al., who also noted that spectrograms are computationally cheaper, given their already reduced and compacted representation~\cite{dieleman2014end}. Raw audio, especially when sampled at a high rate of \SI{44}{\kilo\hertz}, increases the amount of striding width and window sizes of the employed convolutional layers.
Deng et al. reported noticeably fewer speech recognition errors using large-scale deep neural networks when using mel-scale filter bank spectrograms, compared to MFCC features~\cite{deng2013recent}.

\subsection{Language Identification Using i-Vector Systems}
\label{sec:i-vector-systems}
Prior to the neural-network-based deep learning systems mentioned above, many researchers focused on so-called \emph{identity vector} systems (shortly, \emph{i-vector} systems) for spoken-language processing tasks. Dehak et al. introduced i-vector systems for speaker verification tasks~\cite{dehak2011front}. i-vectors are a representation obtained by mapping a sequence of frames of a given utterance onto a low-dimensional vector space, based on a factor analysis technique. This is referred to as the \emph{total variability space}. Typically, such a system is designed as follows. First, a feature extractor is used to turn an audio file into a \num{20}-dimensional MFCC vector or a \num{56}-dimensional \emph{shifted delta cepstral} vector (SDC). Both vectors are then used to train a \emph{unified background model} (UBM), a speaker- and language-independent \emph{Gaussian mixture model} (GMM). Different speakers are clustered into different acoustical subspaces within the UBM. The trained, high-dimensional GMM supervector is then decomposed into its individual components to obtain respective speaker- and language-dependent characteristics. For this purpose, matrix decomposition methods such as \emph{principal component analysis} (\ac{pca}) are employed. During decomposition, the i-vector is whitened by subtracting a global mean, scaled by the inverse square root of a global covariance matrix, and then normalized to unit length~\cite{garcia2011analysis}. Typically, the i-vector is a compact representation of \num{400}~to \num{600}~dimensions. Finally, a score between the model and the i-vector of a test sample is computed. The simplest scoring measure is the cosine distance between the embedded i-vectors of the analyzed languages.

Other research experimented with \emph{linear discriminant analysis} (LDA) and \emph{neighborhood component analysis} (NCA) for dimensionality reduction of the UBM~\cite{dehak2011front}. Sizov et al. reported using \emph{probabilistic linear discriminant analysis} (PLDA) for this purpose~\cite{sizov2016discriminating}.

While many systems share i-vectors as the input to a classifier, the actually used classification algorithms vary widely. In the context of language identification, Dehak et al. used \emph{support vector machines} (SVM) with cosine kernels~\cite{dehak2011front}. Other researchers employed logistic regression~\cite{martinez2011language} or used simple, three-layer neural networks~\cite{plchot2016bat}. Gonzalez et al. outperformed all previous attempts by using a four-layer deep neural network~\cite{gonzalez2015frame}. Gelly et al. presented the use of bidirectional LSTMs with i-vectors~\cite{gelly2016language}. Other submissions to the NIST LRE 2015 challenge~\cite{lre2015} include many further research approaches with i-vector systems, among them using stacked bottleneck features, Bayesian unit discovery, and model fusions~\cite{lee20162015, torres2008mitll, ng2016sheffield}. The extent of feature engineering around i-vectors results in more complex systems with an increasing number of computational steps in their pipeline. Zazo et al. did a comparison analysis between i-vector systems and LSTM networks and found the latter to perform better in some settings and to be on par in others~\cite{zazo2016evaluation}. They also note that the LSTM network used about \SI{85}{\percent} fewer parameters than classical i-vector systems, while achieving robust and comparable results in several challenging scenarios.

\subsection{Methods for Data Augmentation}
Last, we present work related to \emph{data augmentation}. To overcome insufficient varieties in datasets or to extend their sizes, researchers supplement their data with artificially created or augmented files. This approach not only boosts the overall dataset size but helps to improve a model's ability to generalize by learning from more diverse samples. For computer vision tasks, Szegedy et al. proposed to scale, translate, rotate, deform, and mirror the input images~\cite{szegedy2015going}. All these modifications are, however, some form of image manipulation. For audio data, there are alternative approaches, too. Similar to image scaling, one can change the playback speed of audio data by changing the sampling rate. There are two downsides to this approach, though. First, randomized time stretching modifies the audio pitch. Speeding up audio leads to unnaturally high-pitched voices. Second, manipulating the time domain alters the duration of pauses between words and frequency activations. Such extreme modifications could render the augmented data incompatible with the original dataset. Ko et al. recommended changing the speed of the audio signal to no less than \SI{90}{\percent} and no more than \SI{110}{\percent}~of the original signal speed~\cite{ko2015audio}. The largest benefit of this augmentation method are its simplicity and low implementation effort.

Amodei et al. noted that introducting background noise into their data helped to improve speech recognition robustness for noisy speech samples in general~\cite{amodei2015deep}. They augmented about \SI{40}{\percent}~of their dataset with randomly selected audio clips. We explore this approach in our system as well (see Section~\ref{sec:music_robustness}). Cui et al. proposed using a stochastic feature mapping in order to transfer one speaker's speech features to another speaker~\cite{cui2015data}. Languages such as Bengali and Assam are spoken by small communities only and, hence, have only a limited supply of digital available speech recordings. The proposed approach attempts to artificially extend the amount of available sound samples by applying voice conversion.
Jaitly et al. introduced what they call \emph{vocal tract length perturbation} (VTLP) to improve speech recognition systems~\cite{jaitly2013vocal}. Inspired by previous work on \emph{vocal tract length normalization}~\cite{eide1996parametric}, which removes speaker-to-speaker variations using normalization methods, Jaitly et al. warped the frequency of each utterance by a random factor. Using VTLP, recognition errors on the evaluation dataset could be decreased.
