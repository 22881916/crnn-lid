\section{Related Work}
\label{sec:related_work}

-wavenet?

Szegedy et al. reported several iterations of Google's convolutional neural network architecture, called GoogLeNet or Inception\cite{szegedy2015going, szegedy2016rethinking, szegedy2016inception}. These deep and very deep convolutional neural networks repeatedly set the state of the art record for minimal classification errors in the ImageNet Large Scale Visual Recognition Competition (ILSVRC). The presented network architectures present a number of advantages to previous CNN designs such as VGG. They introduced so called inception modules or mini networks that aim to factorize convolutions with larger filter size in order to accelerate training time and reduce the amount of model parameters. The idea is to replace larger spatial filter (e.g 5$\times$5 and 7$\times$7) which are disproportionally expensive in terms of computation with less expensive smaller inception modules with any loss of visual expressiveness. The inception modules are represented as a sequence of 3$\times$3 convolutions followed by a 1$\times$1 convolution. In case of 5$\times$5 filters they were able to achieve a gain of 28\% in computational speed up. The resulting Inception-v2 and Inception-v3 networks consist of 42 layers and are trained using the RMSProp\cite{tieleman2012lecture} optimizer. Compared to the VGG style networks they feature a lower overall computational cost while boosting better accuracy on image vision tasks.

A second innovation introduced by the inception networks is the use of a technique called batch normalization\cite{ioffe2015batch}. Training deep neural networks is complicated by the fact that the distribution of each layer's input changes during training, as the parameters of the previous layer change. This slows down trainings by requiring lower learning rates and careful parameter initialization. They refer to this phenomenon as internal covariate shift. The proposed solution to this problem is to normalize the inputs of each mini batch for the following layer. This results into a number of benefits. Foremost they were able to drastically reduce the time need to converge their models. Batch normalization enabled them to use higher learning rates without running into the vanishing or exploding gradient problem. Furthermore batch normalization acts as model regularizer positively affecting the generalization ability of the network. In turn this eliminates the need for dropout layers as regularizers. They conclude that simply by adding batch normalization to the convolutional layers of the inception network they were able to beat the state of the art of the ILSVRC challenge.



Shi et al.\cite{shi2016end} proposed an end-to-end trainable neural network for image-based sequence recognition and its application to scene text recognition. In their approach they used a hybrid neural network consisting of a convolutional part and a recurrent part for optical character recognition (OCR). They used convolutional layers as robust feature extractors for the input images and interpreted the resulting feature maps as a sequence of feature vectors. This sequence is fed into a LSTM network to capture the contextual information within the sequence. The whole CRNN was jointly trained with a connectionist temporal classification (CTC) loss \cite{graves2006connectionist} to output a sequence of letters. The found a VGG-based network architecture of seven convolutional layers with max pooling followed by a bidirectional LSTM to work best. They report that the use of batch normalization greatly accelerated their training time. Additionally they employed 1$\times$2 sized rectangular pooling windows instead of conventional squared ones. This tweak yielded feature maps of larger width and hence longer sequences for the RNN.

Song et al.\cite{song2015end} reported the use of an end-to-end trainable hybrid convolutional recurrent neural network for automatic speech recognition (ASR). They focussed on classifying a phoneme sequence of the input speech sample as part of an ASR system. Their proposed network consists of four convolutional layers, followed by two fully connected layers and is finalized by two LSTM layers. The network was jointly trained on the TIMIT dataset using CTC loss and used mel-filter bank greyscale images as input. Given the short length of the individual phonemes they operates on audio snippets of 15-25 milliseconds. Similarly to Shi et al. they resort to using rectangular pooling layers to obtain longer feature vector sequences. Their reported results are competitive to traditional gaussian mixture models and hidden markov models for ASR tasks. 

    \subsubsection{A UNIFIED DEEP NEURAL NETWORK FOR SPEAKER AND LANGUAGE RECOGNITION}
    \begin{itemize}
        \item \cite{richardson2015unified}
        \item Almost identical: Deep Neural Network Approaches to Speaker and Language Recognition \cite{richardson2015deep}
        \item high level overview of i-vector system
        \item Task: Language Recogniton + Speaker Recognition
        \item use bottleneck feature in the second to last layer
        \item input 7 static cepstra appended with 49 SDC
        \item DNN has 7 hidden layers of 1024 nodes each with the exception of the 6th bottleneck layer which has 64 nodes
        \item LRE11
    \end{itemize}
    
    \subsubsection{EXTRACTING DEEP NEURAL NETWORK BOTTLENECK FEATURES USING LOW-RANK MATRIX FACTORIZATION
    }
    \begin{itemize}
        \item \cite{zhang2014extracting}
        \item bottle neck feature improve classification results
        \item Task: Automatc Speech Recogniton (ASR)
        \item get bottleneck feature by low rank matrix factorization
        \item this is done by replacing the usual softmax layer weights by a linear layer with a small number of hidden units followed by a softmax layer
        \item BN laier is always last layer
        \item linear layer = FC without activation func
        \item uses DNN with 5 FCs with 1024 hidden units each + sigmoid activations + 1 BN layer
        \item softmax cross entropy loss 
        \item 23 critical-band energies are obtained from a Mel filter-bank, with conversation-side-based mean subtraction = 150 dimensions
        \item further reduciton of output by PCA
        \item hybrid system of DNN + BN feeding into DNN + BN
        
    \end{itemize}

\subsection{LRE 2015}

    \subsubsection{BAT System Description for NIST LRE 2015}
    \begin{itemize}
        \item \cite{plchot2016bat}
        \item participate in the "Fixed" and "Open" LRE Challenge
        \item segment data using automated Voice Activity Detection = previously trained NN
        \item 3042 segments (248 hours of speech) in train set and 42295 segments (146 hours of speech) in dev set.
        \item inputs 24 log Mel-scale filter bank outputs augmented with fundamental frequency features from 4 different f0 esti- mators
        \item used i-vector system
        
    \end{itemize}
    
    \subsubsection{Discriminating Languages in a Probabilistic Latent Subspace}
    \begin{itemize}
        \item \cite{sizovdiscriminating}
        \item Probabilistic Linear Discriminant Analysis (PLDA) model
        \item In this paper, we review state-of-the-art generative methods, based on the Total Variability (TV) model [10], with the aim to improve their performance with discriminative fine-tuning of each language cluster at a time.
        \item TV maps audio into single low-dimensional vector, i-vector, that contains speaker, channel, and phonetic variability
        
    \end{itemize}
    
    \subsubsection{Evaluation of an LSTM-RNN System in Different NIST Language Recognition Frameworks}
    \begin{itemize}
        \item \cite{zazo2016evaluation}
        \item used a one directional LSTM
        \item perform significantly better than i-vectors systems in LRE
        \item nice high level description of how LSTMs work
        \item inputs: random chunks of 2 seconds from which MFCC-SDC (Shifted Delta Coefficients) 
        \item softmax cross entropy loss
        \item use last frame for scoring
        \item comparison if i-vector baseline to LSTM
        \item only used training data for 8 languages with more than 200hours of data
        \item US English (eng), Span- ish (spa), Dari (dar), French (fre), Pashto (pas), Russian (rus), Urdu (urd), Chinese Mandarin (chi)
        \item data split into 3, 10 and 30 seconds
        \item model: two hidden layers of 512 units followed by an output layer. The hidden layers are uni-directional LSTM layers while the output layer is a softmax with as many units as languages in the cluster
        \item LSTM os only better for short utterance (<= 10s)
        \item LSTM uses less parameters than i-vector
        
    \end{itemize}
    
    \subsubsection{Frame-by-frame language identification in short utterances using deep neural networks}
    \begin{itemize}
        \item \cite{gonzalez2015frame}
        \item highlights the downsides/disadvantages of i-vector systems
        
    \end{itemize}
    
    
    
    