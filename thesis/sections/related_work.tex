\section{Related Work}
\label{sec:related_work}
In this chapter we lay out related work concerning neural network designs in general and hybrid networks which are specifically used for language identification. Additionally, we highlight related work on suitable input feature representations for machine learning on audio files. Furthermore, we present research on i-vector systems, the traditional approach to LID. Finally, we list related work on data augmentation. 

\subsection{Convolutional Neural Network Architectures}
With good results in the ImageNet Large Scale Visual Recognition Challenge (ILSVRC)\cite{ILSVRC15}, the \emph{AlexNet}\cite{krizhevsky2012imagenet}, \emph{VGGNet}\cite{simonyan2014very} and \emph{GoogLeNet}/\emph{Inception}\cite{szegedy2015going} have become the de-facto standard for neural network designs in the computer vision community.\\ 
Oxford university's visual geometry group published their deep neural network design which they called \emph{VGGNet}. Simonyan et al. proposed a convolutional neural network architecture with a depth of up to 16 or 19 weight layers.\cite{simonyan2014very, Chatfield14} They presented a thorough evaluation of increasing neural network depth using convolutional layers with 3$\times$3 kernel sizes and ReLU activations. Their system ended up winning the ILSVRC 2104 challenge.\\
Szegedy et al. reported on several iterations of Google's convolutional neural network architecture, called \emph{GoogLeNet} or \emph{Inception}.\cite{szegedy2015going, szegedy2016rethinking, szegedy2016inception} These deep and very deep convolutional neural networks repeatedly set the state-of-the-art record for minimal classification errors in the ImageNet competition. The presented network architectures present a number of advantages to previous CNN designs such as \emph{VGGNet}. Google introduced so-called inception modules or mini networks that aim to factorize convolutions with larger filter size in order to accelerate training time and reduce the amount of model parameters. The idea is to replace larger spatial filters (e.g 5$\times$5 and 7$\times$7) which are disproportionally expensive, in terms of computation with less expensive smaller inception modules with any loss of visual expressiveness. The inception modules are represented as a sequence of 3$\times$3 convolutions followed by a 1$\times$1 convolution. In case of replacing 5$\times$5 filters they were able to achieve a gain of 28\% in computational speed up. The resulting \emph{Inception-v2} and \emph{Inception-v3} networks consist of 42 layers and are trained using the RMSProp\cite{tieleman2012lecture} optimizer. Compared to the \emph{VGGNet}-style networks they feature a lower overall computational cost while offering higher accuracy on image vision tasks.

A second innovation introduced by the inception networks is the use of a technique called \emph{batch normalization}.\cite{ioffe2015batch} Training deep neural networks is particularly complicated through changes in the value distribution of each layer's input during training, as the parameters of the previous layer change. This slows down trainings by requiring lower learning rates and careful parameter initialization. Szegedy et al. refer to this phenomenon as \emph{internal covariate shift}. The proposed solution to this problem is to normalize the inputs of each mini batch for the following layer to unit vectors. This results in a number of benefits: Foremost they were able to drastically reduce the time need to converge their models. Batch normalization enabled them to use higher learning rates without running into the \emph{vanishing} or \emph{exploding gradient problem}. Furthermore, batch normalization acts as model regularizer positively affecting the generalization abilities of the network. In turn, this eliminates the need for dropout layers as regularizers. They conclude that simply by adding batch normalization to the convolutional layers of the inception network they were able to beat the state of the art of the ILSVRC challenge.

Shi et al. proposed an end-to-end trainable neural network for image-based sequence recognition and its application to scene text recognition.\cite{shi2016end} In their approach they used a hybrid neural network consisting of a convolutional part and a recurrent part for optical character recognition (OCR). The authors used convolutional layers as robust feature extractors for the input images and interpreted the resulting feature maps as a sequence of feature vectors. This sequence is fed into a LSTM network to capture the contextual information within the sequence. The whole CRNN was jointly trained with a \emph{connectionist temporal classification} (CTC) loss\cite{graves2006connectionist} to output a sequence of letters. They found a \emph{VGGNet}-based network architecture of seven convolutional layers with max pooling followed by a bidirectional LSTM to work best. The paper concludes that the use of \emph{batch normalization} greatly accelerated their training time. Additionally the authors employed 1$\times$2-sized rectangular pooling windows instead of conventional squared ones. This tweak yielded feature maps of larger width and hence longer sequences for the RNN.

\subsection{Spoken Language Processing Systems}
Much of the research around audio representations and spoken language processing is rooted in various related research communities: Music information retrieval (MIR), Automatic Speech Recognition (ASR) and Language Identification (LID). Alongside the rise in popularity of neural networks within the computer vision community we can witness their use for audio based tasks as well. Early LID system integrate and combine shallow neural networks within their i-vector systems (more details below).\cite{gonzalez2014automatic, han2013trap, matejka2014neural, richardson2015unified} These models usually feature no more than three layers and consist solely of classic neural networks or fully connected layers. While these systems neither benefit from the more efficient computation of CNNs nor the expressiveness of deeper systems they already were able to improve existing systems.

Song et al. reported the use of an end-to-end trainable, hybrid, convolutional recurrent neural network for automatic speech recognition (ASR).\cite{song2015end} They focussed on classifying a phoneme sequence of the input speech sample as part of their ASR system. Their proposed network consists of four convolutional layers, followed by two fully connected layers and is finalized by two LSTM layers. The network was jointly trained on the TIMIT dataset using CTC loss and mel-filter bank greyscale images as input. Given the short length of the individual phonemes they operates on audio snippets of 15-25 millisecond duration. Similarly to Shi et al. they apply rectangular pooling layers to obtain longer feature vector sequences.\cite{shi2016end} The reported results are competitive to traditional gaussian mixture models and hidden markov models for ASR tasks. 

Amodei et al. presented the Deep Speech 2 system for speech recognition. The system supports English and Mandarin Chinese as input languages.\cite{amodei2015deep} Their CRNN employed only three convolutional layers followed by seven bidirectional RNN or GRU layers. The model was trained end-to-end using CTC loss and a sequence of spectrograms of power-normalized audio as input. The model outputs a sequence of graphemes and uses a language model together with beam search to reconstruct words from the audio. The authors found both the LSTMs and GRU cells performed similarly well as the RNN layers. GRU cells, however, needed less computations and were less likely to diverge. For the CNN part the paper evaluated both the use of 1D time-only domain convolutions and 2D frequency-time domain convolutions. 2D Convolutions fared better especially with regard to noisy data.\\ 
Deep Speech 2 was trained with 12.000 hours of English Speech and 9.000 hours of Mandarin Chinese. Additionally, it used an increased dataset with augmented data boosting both their effective size of corpus as well as improving their noise robustness. For evaluation the authors also tested the system on accented speech and noisy speech read in coffee shops, streets, etc. In both scenarios the model's performance deteriorated.

\subsection{Methods of Input Data Representation}
There are many different types of audio representations used for spoken language processing. Many higher-level characteristics of sound relate to the energies in different frequency bands. This explains the utility of time-frequency representations of audio such as spectrograms, which are frequently used in literature.\cite{montavon2009deep, dieleman2013multiscale, lee2009unsupervised, wulfing2012unsupervised, henaff2011unsupervised} Alternatively, classical machine learning systems and i-vector systems usually rely on \emph{Mel-Frequency Cepstral Coeffiecient} (\emph{MFCC}) vectors\cite{richardson2015unified, dehak2011front, garcia2011analysis} or derivatives thereof such as perceptual linear prediction (PLP) coefficients\cite{gonzalez2014automatic}. Others have evaluated the use of raw waveform audio directly\cite{dieleman2014end, collobert2016wav2letter}.\\
Collobert et al. introduced Wav2Letter, an end-to-end convolutional neural network speech recognition.\cite{collobert2016wav2letter} They evaluated their system with three different input representations: Mel-Frequency Cepstral Coeffiecient (MFCC), spectrograms and raw waveform data. Here, a fully convolutional neural network performed best with MFCC vectors as input. Yet, power spectrograms still outperformed raw waveform inputs. This observation corresponded with Dieleman et al. who also noted that spectrograms are computationally cheaper given their already reduced and compacted representation.\cite{dieleman2014end} Raw audio, especially when sampled at a high rate of 44kHz, would increase the amount of striding width and window sizes of the employed convolutional layers.  
Deng et al. reported noticeably lower speech recognition errors using large-scale deep neural networks when using mel-scale filterbank spectrograms compared to MFCC features.\cite{deng2013recent}

\subsection{Language Identification Using i-vector Systems}
Prior to the neural-network-based deep learning systems mentioned above many researchers focussed on so-called identity vector (\emph{i-vector}) systems for spoken language processing tasks. Dehak et al. introduced i-vector systems for speaker verification tasks.\cite{dehak2011front} I-vectors are a representation obtained by mapping a sequence of frames of a given utterance into a low-dimensional vector space based on a factor analysis technique. This is referred to as the total variability space.  Typically such a system is designed as follows. First, a feature extractor is used to turn an audio file into a 20 dimensional mel-frequency cepstral coefficient (MFCC) vector or a 56 dimensional shifted delta cepstral (SDC) vector. These are then used to train a unified background model (UBM), a speaker and language-independent Gaussian mixture model (GMM). Different speakers have different subspaces within this universal acoustical cluster inside the UBM. The trained, high dimensional GMM supervector is decomposed into its individual components to obtain individual speaker and language dependent characteristics. Matrix decomposition methods such as principal component analysis (PCA) can be used for the decomposition step. For this, the i-vector is whitened by subtracting a global mean, scaled by the inverse square root of a global covariance matrix, and then normalized to unit length.\cite{garcia2011analysis} Typically the i-vector is a compact representation of 400-600 dimensions. Finally, a score between the model and the i-vector of a test sample is computed. The simplest scoring measure is the cosine distance between the embedded i-vectors of languages.

Other research experimented with linear discriminant analysis (LDA) and neighborhood component analysis (NCA) for dimensionality reduction of the UBM.\cite{dehak2011front} Sizov et al. reported the use of probabilistic linear discriminant analysis (PLDA) for this job.\cite{sizov2016discriminating}\\
While many systems share i-vectors as the input to a classifier the actual type of classification algorithm varies widely. For language identification Dehak et al. used support vector machines (SVM) with cosine kernels.\cite{dehak2011front} Others employed logistic regression\cite{martinez2011language} or used a simple three layer neural network\cite{plchot2016bat}. Gonzalez et al. outperformed all previous attempts by using a four layer deep neural network.\cite{gonzalez2015frame} Gelly et al. presented the use of bidirectional LSTMs with i-vectors.\cite{gelly2016language} Other submissions to NIST LRE 2015 challenge\cite{lre2015} include many further research approaches with i-vector systems including the use of stacked bottleneck features, bayesian unit discovery and model fusions.\cite{lee20162015, torres2008mitll, ng2016sheffield} This feature engineering around i-vectors results in more complex systems with an increasing number of processing and computational steps in their pipeline. Zazo et al. did a comparison analysis between i-vector systems and LSTM networks and found the latter to perform better in some settings and to be on par in others.\cite{zazo2016evaluation} They also note, that the LSTM network used about 85\% less parameters than a classical i-vector system while achieving robust and comparable results in several challenging scenarios.

\subsection{Methods For Data Augmentation}
Last, we present related work to data \emph{augmentation}. To overcome insufficient variety in a dataset or to extend its size, researchers supplement their data with artificially created or augmented files. This approach not only boosts the overall dataset size but helps to improve a model's ability to generalize by learning from more diverse samples. For computer vision tasks, Szegedy et al. proposed to scale, translate, rotate, deform and mirror the input images.\cite{szegedy2015going} All these modifications are, however, some form of image manipulation. For audio data there are alternative approaches, too. Similarly to image scaling, one can change the playback speed of audio data by changing the sampling rate. There are two downsides to this approach, though. First, randomized time stretching changes the audio pitch. Speeding up audio results on speech recordings of unnaturally high pitched voices. Second, manipulation the time domain alters the length of pauses between words and frequency activations. Such extreme modifications could render the augmented data incompatible to the original dataset. Ko et al. recommendeD changing the speed of the audio signal to no less than 90\% and no more than 110\% of original signal speed.\cite{ko2015audio} The largest benefit of this augmentation method is its simplicity and low implementation cost.

Amodei et al. noted that the introduction of background noise into their data helped to improve speech recognition robustness for noisy speech samples in general.\cite{amodei2015deep} They augmented about 40\% of their dataset with randomly selected audio clips. We will explore this approach in our system, too. Results of this are documented in a later chapter. Cui et al. proposed the use of stochastic feature mapping in order to transfer one speaker's speech features to another.\cite{cui2015data} Their approach attempted to apply voice conversion for languages like Bengali and Assam, both of which are spoken only by small communities and hence needed to increase the limited amount of available digital speech recordings.\\ 
Jaitly et al.\cite{jaitly2013vocal} introduced what they called \emph{Vocal tract length perturbation} (VTLP) to improve speech recognition systems. Inspired by previous work on Vocal Tract Length Normalization \cite{eide1996parametric}, which removes speaker-to-speaker variations using normalization methods, they warped the frequency of each utterance by a random factor. Using VTLP they were able to decrease the recognition error on their evaluation dataset.

       