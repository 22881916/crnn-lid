\section{Demo Application}
\label{sec:demo}
To showcase some possible use cases for our language identification system, we developed a web service. First, we built a REST interface to our web server that enables a user to upload and identify an audio file. This API could be offered as a standalone service similar to existing speech recognition services such as Google Cloud Speech API\footnote{\url{https://cloud.google.com/speech}, accessed 16 May 2017} or IBM Watson Speech to Text.\footnote{\url{https://www.ibm.com/watson/developercloud/speech-to-text.html}, accessed 16 May 2017} Second, we built a website that allows users to upload an audio file for identification. An interactive results page displays the computed prediction probabilities as charts. Further, we embedded a speech-to-text service, which automatically transcribes the users' audio file to the identified language. As of May 2017, no commercially available automatic speech recognition service is capable of language identification and, hence, all of them require a manual selection of the target language. Our demo application automates this step and preselects the identified language.

The web server was implemented in Python using the Flask micro web development framework.\footnote{\url{http://flask.pocoo.org}, accessed 16 May 2017} The interactive front end is designed as a JavaScript single-page app using the React framework\footnote{\url{https://facebook.github.io/react/}, accessed 16 May 2017} to offer a modular, interactive, and declaratively built user interface. The client-side web application architecture is based on the Flux pattern\footnote{\url{https://facebook.github.io/flux}, accessed 16 May 2017} to enforce a unidirectional data flow, which works well in unison with React's declarative programming style.

The language prediction is handled by the server using Keras and our best-performing CRNN model. For each incoming request, we apply the same preprocessing steps to the audio file as during our training sessions. Audio files are converted to spectrogram images on the fly, before being classified by our deep CRNN system. In contrast to our training data, user-uploaded audio files may extend past our ten-second audio snippet duration. Longer files are split into a sequence of individual ten-second snippets for classification. All snippets with a duration of less than ten seconds are discarded. The resulting prediction probabilities are averaged into a single fusion score. 






