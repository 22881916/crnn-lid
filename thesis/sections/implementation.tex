\section{Implementation}
This section outlines the software resources of our language identification system, explains the necessary data preprocessing, and describes the model architecture of our neural networks in more detail.

\subsection{Software}
\label{sec:software}

	Our language identification system is implemented in Python 3 and uses the open source deep learning framework Keras\cite{chollet2015keras} with the TensorFlow\cite{abadi2016tensorflow} backend for training our neural networks. Keras provides us with a set of higher level machine learning primitives such as convolutional layers and optimization algorithms such as stochastic gradient descent without sacrificing any fine grained control over the training parameters. Internally it builds on Google's open source numerical operation library TensorFlow which is optimized to quickly compute multidimensional data on GPUs. We make heavy use Keras' aforementioned primitives, e.g. convolutional layers and the efficient LSTM implementation. The recently announced\footnote{\url{https://research.googleblog.com/2017/02/announcing-tensorflow-10.html?m=1}, accessed 03.03.2017} version 1.0 of TensorFlow even advertises the ability to generate a small and efficient binary version of our models ready to be deployed on mobile phones.
	
	All models are persisted to disk during training, including a summary of the layer architecture as well as their weights. This makes it easy to load, evaluate and production deploy all models later. During the evaluation phase all performance metrics (accuracy, precision, recall, F1 score) are calculated using the Scikit Learn\cite{scikit-learn} framework. All measurements are logged and visualized using TensorBoard\footnote{\url{https://www.tensorflow.org/how_tos/summaries_and_tensorboard/}, accessed 30.01.2017}, both for the training and validation set. Having the ability to easily study and compare different metrics such as accuracy and loss across several training runs made it very comfortable to continuously monitor the progress of our research. Figure \ref{fig:tensorboard} shows a TensorBoard instance with metric plots for several models being discussed by our team.
	\begin{figure}[]
  		\centering
    	\includegraphics[width=\textwidth,keepaspectratio]{img/tensorboard.png}
    	\caption{Training accuracy and loss of different models visualized in TensorBoard. Plotting multiple evaluation measures over many training runs helped judge the overall performance progress of the system.}
    	\label{fig:tensorboard}
	\end{figure}		
	
\subsection{Data Preprocessing}
\label{sec:data_processing}
All audio files undergo preprocessing before being fed to the neural network. As a first step all files are encoded as uncompressed, lossless Waveform Audio File Format\footnote{\url{http://www.microsoft.com/whdc/device/audio/multichaud.mspx}, accessed 23.02.2017}, WAVE, commonly know by its file extension *.wav. This conversion has two advantages: A lossless data codec allows for future audio manipulations without any deterioration in signal quality and makes the data easily interchangeable for third party programs and library such as SciPy\footnote{\url{https://www.scipy.org/}, accessed 23.02.2017}. 

	Since all our neural networks do not operate on raw waveform audio signals directly we transfer our features into the image domain. As introduced in section \ref{sec:audio_representations} we used a spectrogram representation of the audio file for training our models. The spectrograms were generated using the open source command line tool SoX\footnote{\url{http://sox.sourceforge.net/}, accessed 23.02.2017}. The spectrograms are discretized using a Hann window \todo{source} and 129 frequency bins along the frequency axis (y-axis) as instructed by SoX manual\footnote{\url{http://sox.sourceforge.net/sox.pdf}, page 32, accessed 26.03.2017}. The time axis (x-axis) is rendered at 25 pixel per second. Each audio sequence is clipped into non-overlapping ten second segments. The  final segment is discarded to avoid segments shorter than the required ten seconds. We decided against filtering silent sections within the audio segment to keep the natural pauses between words and not disturb the regular speech rhythm. Frequency intensities are mapped to a gray scale. The resulting greyscale images are saved as lossless PNG files with 500 pixel in width and 129 pixel in height. Appendix \ref{sec:appendix_a} includes the complete listing \ref{lst:spectrograms} for generating a spectrogram image with SoX.
	
	As can be seen in figure \ref{fig:spectrogram} the spectrograms feature very apparent bright ripple-like pattern. Each of these represents a strong activation of a certain frequency over time. Several frequency activations can be active simultaneously constituting a particular phoneme or sound. A sequence of these phonemes forms words and is only interrupted by short speech pauses. We hypothesize that our \ac{lid} system will learn the characteristic and unique composition of these frequency activation for every language in our classifier. 

	
	\begin{figure}[]
  		\centering
    	\includegraphics[width=\textwidth,keepaspectratio]{img/spectrogram.png}
    	\caption{A spectrogram generated from a ten second German audio clip  using SoX. Notice the bright ripple-like patterns representing high frequency activations. We hypothesize that these frequency activations will serve as the main features for the classifier.}
    	\label{fig:spectrogram}
	\end{figure}
	

\subsection{CNN Architecture}
\label{sec:cnn_architecture}
To successfully solve our research task with deep neural networks we had to design a fitting model architecture. Combing the right layers and adjusting the correct parameters is not an easy task. Critics argue that deep learning systems are a black box and that the impact of individual settings is hard to judge. Therefore, we based our model architectures on proven existing designs from related work and adapted them to our needs, while heeding as best practices\cite{szegedy2016rethinking}\cite{mishkin2016systematic}. 
The size of the deep neural network is determined by the bias-variance tradeoff\cite{geman1992neural}, imposing underfitting or overfitting on the resulting model. The bias characterizes the prediction error that results from the model's limited ability to learn relevant relations of features in the training data. Underfitting can be caused by designing a too small network layer causing a high bias. Variance in contrast refers to the error that results from the model responding overly sensitive to variation in the training data. By designing a too large network and introducing too many parameters the model results in low variance. Hence, it learns an exact representation of the training data without abstracting for more general applications. This is known as overfitting.
Designing and adjusting the network layout is an empirical process. We tried many variations and parameters of our proposed model layouts to find the most suitable design for our LID system. The layout of the network interdepends and interacts with other design decisions of the system, especially the loss calculation. It hence needs to be tested in a complete process setup. 

For this thesis we tried two different convolutional neural networks. The first one is based on the early VGG-style model architectures of Simonyan et al. \cite{Chatfield14}. With only five convolutional layers and modest feature map output sizes this is the smaller of two designs. Note the large kernel size of 7x7 pixels and 5x5 pixels for the first two layers yielding a large receptive field. We complemented each convolutional layer with batch normalization\cite{ioffe2015batch}, a technique that both helps with training speed and model regularization. The full network layout is available in table \ref{tab:layers_CNN_A}.

 \begin{table}[]
  \centering
  \begin{tabularx}{\textwidth}{Xccc}
  \toprule
Layer Type                                          & output size    & kernel & stride  \\ \midrule
\mbox{Convolution with} \mbox{Batch Normalization}  & 123 x 494 x 16 & 7x7    & 1       \\ 
Max Pooling                                         & 61 x 247 x 16  & 2x2    & 2       \\ 
\mbox{Convolution with} \mbox{Batch Normalization}  & 57 x 243 x 32  & 5x5    & 1       \\ 
Max Pooling                                         & 28 x 121 x 32  & 2x2    & 2       \\ 
\mbox{Convolution with} \mbox{Batch Normalization}  & 26 x 119 x 64  & 3x3    & 1       \\ 
Max Pooling                                         & 13 x 59 x 64   & 2x2    & 2       \\ 
\mbox{Convolution with} \mbox{Batch Normalization}  & 11 x 57 x 128  & 3x3    & 1       \\ 
Max Pooling                                         & 5 x 28 x 128   & 2x2    & 2       \\ 
\mbox{Convolution with} \mbox{Batch Normalization}  & 3 x 26 x 256   & 3x3    & 1       \\ 
Max Pooling                                         & 1 x 13 x 256   & 2x2    & 2       \\ 
Dropout \& Flatten                                  & 3328           &        &         \\ 
\mbox{Fully Connected}                              & 1024           &        &         \\ 
\mbox{Fully Connected}                              & 4              &        &         \\ 
  \bottomrule
  \end{tabularx}
  \caption{The layerwise architecture for the convolutional neural network CNN\_A.}
  \label{tab:layers_CNN_A}
  \end{table}

The second CNN architecture is based on the VGG-16 network\cite{simonyan2014very} and has considerably more parameters. The CNN is deeper with seven convolutional layers and has increased feature map outputs in an effort to capture more information. In contrast to the first design, all convolutional layers feature equally sized kernels of 3x3 pixels. The complete model architecture can be found in table \ref{tab:layers_CNN_C}.

    
  \begin{table}[]
  \centering
  \begin{tabularx}{\textwidth}{Xccc}
  \toprule
Layer Type                                          & output size    & kernel & stride  \\ \midrule
\mbox{Convolution with} \mbox{Batch Normalization}  & 127 x 498 x 64  & 3x3    & 1x1     \\ 
Max Pooling                                         & 63 x 249 x 64   & 2x2    & 2x2     \\ 
\mbox{Convolution with} \mbox{Batch Normalization}  & 61 x 247 x 128  & 3x3    & 1x1     \\ 
Max Pooling                                         & 30 x 123 x 128  & 2x2    & 2x2     \\ 
\mbox{Convolution with} \mbox{Batch Normalization}  & 28 x 121 x 256  & 3x3    & 1x1     \\ 
\mbox{Convolution with} \mbox{Batch Normalization}  & 26 x 119 x 256  & 3x3    & 1x1     \\ 
Max Pooling                                         & 13 x 59 x 256   & 2x2    & 2x2     \\ 
\mbox{Convolution with} \mbox{Batch Normalization}  & 11 x 57 x 512   & 3x3    & 1x1     \\ 
\mbox{Convolution with} \mbox{Batch Normalization}  & 9 x 55 x 512    & 3x3    & 1x1     \\ 
Max Pooling                                         & 4 x 27 x 512    & 2x2    & 2x2     \\ 
\mbox{Convolution with} \mbox{Batch Normalization}  & 2 x 25 x 512    & 3x3    & 1x1     \\ 
Max Pooling                                         & 1 x 12 x 512    & 2x2    & 2x2     \\ 
Flatten                                             & 6144            &        &         \\                        
\mbox{Fully Connected}                              & 1024            &        &         \\
\mbox{Fully Connected}                              & 4               &        &         \\
  \bottomrule
  \end{tabularx}
  \caption{The layerwise architecture for the convolutional neural network CNN\_C.}
  \label{tab:layers_CNN_C}
  \end{table} 
    

\subsection{CRNN Architecture}

	\todo{Write something here.}
    
  \begin{table}[h]
  \centering
  \begin{tabularx}{\textwidth}{Xccc}
  \toprule
Layer Type                                          & output size    & kernel & stride  \\ \midrule
\mbox{Convolution with} \mbox{Batch Normalization}  & 123 x 494 x 16  & 7x7    & 1x1     \\ 
Max Pooling                                         & 61 x 247 x 16   & 2x2    & 2x2     \\ 
\mbox{Convolution with} \mbox{Batch Normalization}  & 57 x 243 x 32   & 5x5    & 1x1     \\ 
Max Pooling                                         & 28 x 121 x 32   & 2x2    & 2x2     \\ 
\mbox{Convolution with} \mbox{Batch Normalization}  & 26 x 119 x 64   & 3x3    & 1x1     \\ 
Max Pooling                                         & 13 x 59 x 64    & 2x2    & 2x2     \\ 
\mbox{Convolution with} \mbox{Batch Normalization}  & 11 x 57 x 128   & 3x3    & 1x1     \\ 
Max Pooling                                         & 5 x 56 x 128    & 2x2    & 2x1     \\ 
\mbox{Convolution with} \mbox{Batch Normalization}  & 3 x 54 x 256    & 3x3    & 1x1     \\ 
Max Pooling                                         & 1 x 53 x 256    & 2x2    & 2x1     \\ 
Permute                                             & 53 x 1 x 256    &        &         \\
Reshape                                             & 53 x 256        &        &         \\
\mbox{Bidirectional LSTM}                           & 1024            &        &         \\
\mbox{Fully Connected}                              & 4               &        &         \\
  \bottomrule
  \end{tabularx}
  \caption{The layerwise architecture for the convolutional recurrent neural network.}
  \label{tab:layers_CRNN}
  \end{table}
  
  
